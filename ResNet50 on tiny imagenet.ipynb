{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Implemented ResNet32 or 50 architecture (you are allowed to used existing implementation in\n",
    "keras framework) and evaluate the performance of the model on Tiny ImageNet dataset\n",
    "\n",
    "\n",
    "Dataset source: https://www.kaggle.com/akash2sharma/tiny-imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model,Sequential,Model\n",
    "from tensorflow.keras.applications import ResNet50,VGG16,resnet50\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Conv2D, MaxPooling1D,GlobalAveragePooling2D, Embedding, Flatten, Input, add,concatenate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.optimizers import Adadelta,Adam,RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv2.imread(\"/home/kashraf/fall2021_coursework/Neural Nets/Assignment3/tiny_image_net/train/n01641577/n01641577_7.JPEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Resnet50 Archttecure-----------------------------\n",
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 70, 70, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 32, 32, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 32, 32, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 32, 32, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 34, 34, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 16, 16, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 16, 16, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 16, 16, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 16, 16, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 16, 16, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 16, 16, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 16, 16, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 16, 16, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 16, 16, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 16, 16, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 16, 16, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 16, 16, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 16, 16, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 16, 16, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 16, 16, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 16, 16, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 8, 8, 128)    32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 8, 8, 128)    0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 8, 8, 128)    0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 8, 8, 512)    131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 8, 8, 512)    0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 8, 8, 512)    0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 8, 8, 128)    0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 8, 8, 128)    0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 8, 8, 512)    0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 8, 8, 512)    0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 8, 8, 128)    0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 8, 8, 128)    0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 8, 8, 512)    0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 8, 8, 512)    0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 8, 8, 128)    0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 8, 8, 128)    0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 8, 8, 512)    0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 8, 8, 512)    0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 4, 4, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 4, 4, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 4, 4, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 4, 4, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 4, 4, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 4, 4, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 4, 4, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 4, 4, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 4, 4, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 4, 4, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 4, 4, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 4, 4, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 4, 4, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 4, 4, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 4, 4, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 4, 4, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 4, 4, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 4, 4, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 4, 4, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 4, 4, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 4, 4, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 4, 4, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 4, 4, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 4, 4, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 2, 2, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 2, 2, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 2, 2, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 2, 2, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 2, 2, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 2, 2, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 2, 2, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 2, 2, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 2, 2, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 2, 2, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 2, 2, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 2, 2, 2048)   0           conv5_block3_add[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 23,534,592\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape=tf.keras.Input(shape=(64,64,3))\n",
    "res_model=ResNet50(include_top=False, weights=None,input_tensor=input_shape)\n",
    "print(\"----------------------Resnet50 Archttecure-----------------------------\")\n",
    "print(res_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's add full connected layer to our model and softmax activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2, 2, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200)               409800    \n",
      "=================================================================\n",
      "Total params: 23,997,512\n",
      "Trainable params: 23,944,392\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(res_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(200, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70000 images belonging to 200 classes.\n",
      "Found 30000 images belonging to 200 classes.\n",
      "Train shape: (64, 64, 3)\n",
      "Test shape: (64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "scale=1./255\n",
    "train_dir=\"/home/kashraf/fall2021_coursework/Neural Nets/Assignment3/tiny_image_net/train/\"\n",
    "test_dir=  \"/home/kashraf/fall2021_coursework/Neural Nets/Assignment3/tiny_image_net/test/\"\n",
    "\n",
    "train_gen=ImageDataGenerator(rescale=scale)\n",
    "test_gen=ImageDataGenerator(rescale=scale)\n",
    "\n",
    "\n",
    "train_generator=train_gen.flow_from_directory(train_dir,target_size=(64,64),batch_size=32)\n",
    "test_generator=test_gen.flow_from_directory(test_dir,target_size=(64,64),batch_size=32)\n",
    "\n",
    "\n",
    "print(\"Train shape:\",train_generator.image_shape)\n",
    "print(\"Test shape:\",test_generator.image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 5.8495 - accuracy: 0.0135\n",
      "Epoch 00001: val_loss improved from inf to 5.59056, saving model to Resnet50_V2.h5\n",
      "2188/2188 [==============================] - 72s 33ms/step - loss: 5.8490 - accuracy: 0.0135 - val_loss: 5.5906 - val_accuracy: 0.0262 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 4.9350 - accuracy: 0.0343\n",
      "Epoch 00002: val_loss did not improve from 5.59056\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 4.9350 - accuracy: 0.0343 - val_loss: 32.2069 - val_accuracy: 0.0264 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 4.5436 - accuracy: 0.0711\n",
      "Epoch 00003: val_loss did not improve from 5.59056\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 4.5436 - accuracy: 0.0712 - val_loss: 5.9232 - val_accuracy: 0.0889 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 4.2953 - accuracy: 0.1035\n",
      "Epoch 00004: val_loss did not improve from 5.59056\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 4.2953 - accuracy: 0.1035 - val_loss: 10.7675 - val_accuracy: 0.1211 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 3.7202 - accuracy: 0.1777\n",
      "Epoch 00005: val_loss did not improve from 5.59056\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 3.7203 - accuracy: 0.1777 - val_loss: 5.6106 - val_accuracy: 0.2052 - lr: 2.0000e-04\n",
      "Epoch 6/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 3.4967 - accuracy: 0.2107\n",
      "Epoch 00006: val_loss did not improve from 5.59056\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 3.4967 - accuracy: 0.2107 - val_loss: 6.7078 - val_accuracy: 0.2247 - lr: 2.0000e-04\n",
      "Epoch 7/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 3.3312 - accuracy: 0.2361\n",
      "Epoch 00007: val_loss improved from 5.59056 to 3.90370, saving model to Resnet50_V2.h5\n",
      "2188/2188 [==============================] - 73s 33ms/step - loss: 3.3312 - accuracy: 0.2361 - val_loss: 3.9037 - val_accuracy: 0.2451 - lr: 2.0000e-04\n",
      "Epoch 8/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 3.1867 - accuracy: 0.2629\n",
      "Epoch 00008: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 3.1867 - accuracy: 0.2629 - val_loss: 9.3434 - val_accuracy: 0.2302 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 3.0441 - accuracy: 0.2874\n",
      "Epoch 00009: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 3.0442 - accuracy: 0.2874 - val_loss: 6.4496 - val_accuracy: 0.2687 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.8961 - accuracy: 0.3123\n",
      "Epoch 00010: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.8961 - accuracy: 0.3123 - val_loss: 79.4818 - val_accuracy: 0.2703 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.6111 - accuracy: 0.3636\n",
      "Epoch 00011: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.6111 - accuracy: 0.3636 - val_loss: 52.6773 - val_accuracy: 0.2868 - lr: 4.0000e-05\n",
      "Epoch 12/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.5082 - accuracy: 0.3835\n",
      "Epoch 00012: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.5081 - accuracy: 0.3836 - val_loss: 69.5988 - val_accuracy: 0.2913 - lr: 4.0000e-05\n",
      "Epoch 13/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.4416 - accuracy: 0.3953\n",
      "Epoch 00013: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.4416 - accuracy: 0.3953 - val_loss: 103.9695 - val_accuracy: 0.2921 - lr: 4.0000e-05\n",
      "Epoch 14/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.3444 - accuracy: 0.4160\n",
      "Epoch 00014: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.3444 - accuracy: 0.4160 - val_loss: 99.5274 - val_accuracy: 0.2940 - lr: 8.0000e-06\n",
      "Epoch 15/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.3256 - accuracy: 0.4177\n",
      "Epoch 00015: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.3256 - accuracy: 0.4177 - val_loss: 115.1953 - val_accuracy: 0.2930 - lr: 8.0000e-06\n",
      "Epoch 16/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.3088 - accuracy: 0.4214\n",
      "Epoch 00016: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.3091 - accuracy: 0.4213 - val_loss: 157.9857 - val_accuracy: 0.2908 - lr: 8.0000e-06\n",
      "Epoch 17/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2880 - accuracy: 0.4272\n",
      "Epoch 00017: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.2880 - accuracy: 0.4271 - val_loss: 149.7254 - val_accuracy: 0.2919 - lr: 1.6000e-06\n",
      "Epoch 18/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2879 - accuracy: 0.4272\n",
      "Epoch 00018: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.2881 - accuracy: 0.4271 - val_loss: 116.1144 - val_accuracy: 0.2928 - lr: 1.6000e-06\n",
      "Epoch 19/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2835 - accuracy: 0.4270\n",
      "Epoch 00019: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.2835 - accuracy: 0.4270 - val_loss: 191.8483 - val_accuracy: 0.2914 - lr: 1.6000e-06\n",
      "Epoch 20/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2756 - accuracy: 0.4294\n",
      "Epoch 00020: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.2756 - accuracy: 0.4294 - val_loss: 153.8519 - val_accuracy: 0.2924 - lr: 3.2000e-07\n",
      "Epoch 21/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2797 - accuracy: 0.4261\n",
      "Epoch 00021: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.2793 - accuracy: 0.4261 - val_loss: 219.7847 - val_accuracy: 0.2911 - lr: 3.2000e-07\n",
      "Epoch 22/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2778 - accuracy: 0.4273\n",
      "Epoch 00022: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.2778 - accuracy: 0.4273 - val_loss: 126.0850 - val_accuracy: 0.2923 - lr: 3.2000e-07\n",
      "Epoch 23/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2694 - accuracy: 0.4306\n",
      "Epoch 00023: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 70s 32ms/step - loss: 2.2692 - accuracy: 0.4307 - val_loss: 159.5469 - val_accuracy: 0.2924 - lr: 6.4000e-08\n",
      "Epoch 24/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2790 - accuracy: 0.4269\n",
      "Epoch 00024: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 70s 32ms/step - loss: 2.2788 - accuracy: 0.4269 - val_loss: 161.6739 - val_accuracy: 0.2921 - lr: 6.4000e-08\n",
      "Epoch 25/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2709 - accuracy: 0.4301\n",
      "Epoch 00025: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "2188/2188 [==============================] - 70s 32ms/step - loss: 2.2710 - accuracy: 0.4301 - val_loss: 143.0459 - val_accuracy: 0.2926 - lr: 6.4000e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2768 - accuracy: 0.4293\n",
      "Epoch 00026: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2769 - accuracy: 0.4293 - val_loss: 108.7459 - val_accuracy: 0.2936 - lr: 1.2800e-08\n",
      "Epoch 27/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2726 - accuracy: 0.4280\n",
      "Epoch 00027: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2727 - accuracy: 0.4280 - val_loss: 140.4881 - val_accuracy: 0.2929 - lr: 1.2800e-08\n",
      "Epoch 28/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2735 - accuracy: 0.4278\n",
      "Epoch 00028: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2734 - accuracy: 0.4278 - val_loss: 132.8247 - val_accuracy: 0.2928 - lr: 1.2800e-08\n",
      "Epoch 29/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2746 - accuracy: 0.4294\n",
      "Epoch 00029: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2745 - accuracy: 0.4294 - val_loss: 148.3435 - val_accuracy: 0.2923 - lr: 2.5600e-09\n",
      "Epoch 30/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2765 - accuracy: 0.4267\n",
      "Epoch 00030: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.2765 - accuracy: 0.4267 - val_loss: 131.5677 - val_accuracy: 0.2933 - lr: 2.5600e-09\n",
      "Epoch 31/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2739 - accuracy: 0.4283\n",
      "Epoch 00031: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
      "2188/2188 [==============================] - 70s 32ms/step - loss: 2.2739 - accuracy: 0.4283 - val_loss: 154.8079 - val_accuracy: 0.2925 - lr: 2.5600e-09\n",
      "Epoch 32/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2793 - accuracy: 0.4279\n",
      "Epoch 00032: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 70s 32ms/step - loss: 2.2795 - accuracy: 0.4279 - val_loss: 138.6308 - val_accuracy: 0.2927 - lr: 5.1200e-10\n",
      "Epoch 33/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2738 - accuracy: 0.4289\n",
      "Epoch 00033: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 70s 32ms/step - loss: 2.2738 - accuracy: 0.4289 - val_loss: 171.3573 - val_accuracy: 0.2922 - lr: 5.1200e-10\n",
      "Epoch 34/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2738 - accuracy: 0.4281\n",
      "Epoch 00034: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n",
      "2188/2188 [==============================] - 70s 32ms/step - loss: 2.2738 - accuracy: 0.4281 - val_loss: 144.9760 - val_accuracy: 0.2928 - lr: 5.1200e-10\n",
      "Epoch 35/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2726 - accuracy: 0.4270\n",
      "Epoch 00035: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 70s 32ms/step - loss: 2.2726 - accuracy: 0.4270 - val_loss: 160.1311 - val_accuracy: 0.2927 - lr: 1.0240e-10\n",
      "Epoch 36/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2759 - accuracy: 0.4268\n",
      "Epoch 00036: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 70s 32ms/step - loss: 2.2760 - accuracy: 0.4267 - val_loss: 159.5776 - val_accuracy: 0.2915 - lr: 1.0240e-10\n",
      "Epoch 37/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2727 - accuracy: 0.4299\n",
      "Epoch 00037: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 2.0480002416167767e-11.\n",
      "2188/2188 [==============================] - 70s 32ms/step - loss: 2.2727 - accuracy: 0.4299 - val_loss: 125.1295 - val_accuracy: 0.2924 - lr: 1.0240e-10\n",
      "Epoch 38/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2763 - accuracy: 0.4283\n",
      "Epoch 00038: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 70s 32ms/step - loss: 2.2765 - accuracy: 0.4283 - val_loss: 159.8116 - val_accuracy: 0.2925 - lr: 2.0480e-11\n",
      "Epoch 39/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2795 - accuracy: 0.4293\n",
      "Epoch 00039: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 70s 32ms/step - loss: 2.2797 - accuracy: 0.4292 - val_loss: 170.3761 - val_accuracy: 0.2923 - lr: 2.0480e-11\n",
      "Epoch 40/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2743 - accuracy: 0.4319\n",
      "Epoch 00040: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 4.096000622011431e-12.\n",
      "2188/2188 [==============================] - 70s 32ms/step - loss: 2.2743 - accuracy: 0.4319 - val_loss: 167.7157 - val_accuracy: 0.2918 - lr: 2.0480e-11\n",
      "Epoch 41/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2750 - accuracy: 0.4281\n",
      "Epoch 00041: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 70s 32ms/step - loss: 2.2751 - accuracy: 0.4281 - val_loss: 158.3241 - val_accuracy: 0.2925 - lr: 4.0960e-12\n",
      "Epoch 42/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2728 - accuracy: 0.4297\n",
      "Epoch 00042: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 70s 32ms/step - loss: 2.2725 - accuracy: 0.4298 - val_loss: 157.3754 - val_accuracy: 0.2921 - lr: 4.0960e-12\n",
      "Epoch 43/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2765 - accuracy: 0.4301\n",
      "Epoch 00043: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 8.192000897078167e-13.\n",
      "2188/2188 [==============================] - 70s 32ms/step - loss: 2.2765 - accuracy: 0.4301 - val_loss: 125.7099 - val_accuracy: 0.2930 - lr: 4.0960e-12\n",
      "Epoch 44/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2746 - accuracy: 0.4277\n",
      "Epoch 00044: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.2746 - accuracy: 0.4277 - val_loss: 132.3586 - val_accuracy: 0.2925 - lr: 8.1920e-13\n",
      "Epoch 45/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2761 - accuracy: 0.4280\n",
      "Epoch 00045: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.2761 - accuracy: 0.4279 - val_loss: 125.1285 - val_accuracy: 0.2931 - lr: 8.1920e-13\n",
      "Epoch 46/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2712 - accuracy: 0.4292\n",
      "Epoch 00046: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.6384001360475466e-13.\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.2713 - accuracy: 0.4292 - val_loss: 146.3718 - val_accuracy: 0.2926 - lr: 8.1920e-13\n",
      "Epoch 47/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2707 - accuracy: 0.4308\n",
      "Epoch 00047: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.2707 - accuracy: 0.4308 - val_loss: 127.8686 - val_accuracy: 0.2927 - lr: 1.6384e-13\n",
      "Epoch 48/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2782 - accuracy: 0.4296\n",
      "Epoch 00048: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.2781 - accuracy: 0.4296 - val_loss: 132.1521 - val_accuracy: 0.2929 - lr: 1.6384e-13\n",
      "Epoch 49/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2788 - accuracy: 0.4285\n",
      "Epoch 00049: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 3.2768002178849846e-14.\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.2788 - accuracy: 0.4285 - val_loss: 165.8187 - val_accuracy: 0.2922 - lr: 1.6384e-13\n",
      "Epoch 50/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2733 - accuracy: 0.4286\n",
      "Epoch 00050: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.2737 - accuracy: 0.4285 - val_loss: 119.4250 - val_accuracy: 0.2929 - lr: 3.2768e-14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2748 - accuracy: 0.4281\n",
      "Epoch 00051: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2747 - accuracy: 0.4281 - val_loss: 141.5294 - val_accuracy: 0.2924 - lr: 3.2768e-14\n",
      "Epoch 52/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2760 - accuracy: 0.4301\n",
      "Epoch 00052: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 6.553600300244697e-15.\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2761 - accuracy: 0.4301 - val_loss: 143.4003 - val_accuracy: 0.2926 - lr: 3.2768e-14\n",
      "Epoch 53/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2777 - accuracy: 0.4272\n",
      "Epoch 00053: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2776 - accuracy: 0.4272 - val_loss: 133.6542 - val_accuracy: 0.2922 - lr: 6.5536e-15\n",
      "Epoch 54/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2715 - accuracy: 0.4301\n",
      "Epoch 00054: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2717 - accuracy: 0.4301 - val_loss: 142.5379 - val_accuracy: 0.2923 - lr: 6.5536e-15\n",
      "Epoch 55/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2753 - accuracy: 0.4285\n",
      "Epoch 00055: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1.3107200431082805e-15.\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2753 - accuracy: 0.4285 - val_loss: 155.0301 - val_accuracy: 0.2921 - lr: 6.5536e-15\n",
      "Epoch 56/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2729 - accuracy: 0.4267\n",
      "Epoch 00056: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2727 - accuracy: 0.4267 - val_loss: 148.5602 - val_accuracy: 0.2926 - lr: 1.3107e-15\n",
      "Epoch 57/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2732 - accuracy: 0.4294\n",
      "Epoch 00057: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2732 - accuracy: 0.4294 - val_loss: 155.1779 - val_accuracy: 0.2922 - lr: 1.3107e-15\n",
      "Epoch 58/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2721 - accuracy: 0.4312\n",
      "Epoch 00058: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 2.6214401285682084e-16.\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2723 - accuracy: 0.4311 - val_loss: 204.7871 - val_accuracy: 0.2913 - lr: 1.3107e-15\n",
      "Epoch 59/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2735 - accuracy: 0.4284\n",
      "Epoch 00059: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2736 - accuracy: 0.4284 - val_loss: 123.5225 - val_accuracy: 0.2926 - lr: 2.6214e-16\n",
      "Epoch 60/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2726 - accuracy: 0.4288\n",
      "Epoch 00060: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2728 - accuracy: 0.4287 - val_loss: 146.3540 - val_accuracy: 0.2927 - lr: 2.6214e-16\n",
      "Epoch 61/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2762 - accuracy: 0.4295\n",
      "Epoch 00061: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 5.2428803630155353e-17.\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2758 - accuracy: 0.4296 - val_loss: 148.1080 - val_accuracy: 0.2927 - lr: 2.6214e-16\n",
      "Epoch 62/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2788 - accuracy: 0.4278\n",
      "Epoch 00062: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2788 - accuracy: 0.4278 - val_loss: 153.7585 - val_accuracy: 0.2929 - lr: 5.2429e-17\n",
      "Epoch 63/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2751 - accuracy: 0.4284\n",
      "Epoch 00063: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2750 - accuracy: 0.4284 - val_loss: 156.8725 - val_accuracy: 0.2920 - lr: 5.2429e-17\n",
      "Epoch 64/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2740 - accuracy: 0.4291\n",
      "Epoch 00064: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.0485760990728867e-17.\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2739 - accuracy: 0.4291 - val_loss: 142.2579 - val_accuracy: 0.2921 - lr: 5.2429e-17\n",
      "Epoch 65/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2748 - accuracy: 0.4288\n",
      "Epoch 00065: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2748 - accuracy: 0.4288 - val_loss: 154.6412 - val_accuracy: 0.2922 - lr: 1.0486e-17\n",
      "Epoch 66/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2742 - accuracy: 0.4293\n",
      "Epoch 00066: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2742 - accuracy: 0.4293 - val_loss: 125.4058 - val_accuracy: 0.2923 - lr: 1.0486e-17\n",
      "Epoch 67/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2781 - accuracy: 0.4285\n",
      "Epoch 00067: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 2.097152165058549e-18.\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2783 - accuracy: 0.4285 - val_loss: 144.9363 - val_accuracy: 0.2929 - lr: 1.0486e-17\n",
      "Epoch 68/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2758 - accuracy: 0.4279\n",
      "Epoch 00068: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2760 - accuracy: 0.4279 - val_loss: 124.3612 - val_accuracy: 0.2931 - lr: 2.0972e-18\n",
      "Epoch 69/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2790 - accuracy: 0.4271\n",
      "Epoch 00069: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2791 - accuracy: 0.4270 - val_loss: 147.9964 - val_accuracy: 0.2928 - lr: 2.0972e-18\n",
      "Epoch 70/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2777 - accuracy: 0.4284\n",
      "Epoch 00070: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 4.19430449555322e-19.\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2776 - accuracy: 0.4285 - val_loss: 150.5717 - val_accuracy: 0.2927 - lr: 2.0972e-18\n",
      "Epoch 71/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2778 - accuracy: 0.4264\n",
      "Epoch 00071: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2777 - accuracy: 0.4265 - val_loss: 132.1848 - val_accuracy: 0.2924 - lr: 4.1943e-19\n",
      "Epoch 72/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2757 - accuracy: 0.4289\n",
      "Epoch 00072: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2756 - accuracy: 0.4289 - val_loss: 143.4758 - val_accuracy: 0.2925 - lr: 4.1943e-19\n",
      "Epoch 73/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2725 - accuracy: 0.4303\n",
      "Epoch 00073: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 8.388609197901593e-20.\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2725 - accuracy: 0.4303 - val_loss: 153.8117 - val_accuracy: 0.2920 - lr: 4.1943e-19\n",
      "Epoch 74/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2748 - accuracy: 0.4280\n",
      "Epoch 00074: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2751 - accuracy: 0.4279 - val_loss: 159.3154 - val_accuracy: 0.2926 - lr: 8.3886e-20\n",
      "Epoch 75/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2791 - accuracy: 0.4280\n",
      "Epoch 00075: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2791 - accuracy: 0.4280 - val_loss: 136.3195 - val_accuracy: 0.2930 - lr: 8.3886e-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2737 - accuracy: 0.4302\n",
      "Epoch 00076: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 1.6777218395803187e-20.\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2736 - accuracy: 0.4302 - val_loss: 134.6293 - val_accuracy: 0.2930 - lr: 8.3886e-20\n",
      "Epoch 77/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2697 - accuracy: 0.4313\n",
      "Epoch 00077: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2697 - accuracy: 0.4313 - val_loss: 145.1281 - val_accuracy: 0.2923 - lr: 1.6777e-20\n",
      "Epoch 78/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2742 - accuracy: 0.4282\n",
      "Epoch 00078: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2742 - accuracy: 0.4282 - val_loss: 142.9228 - val_accuracy: 0.2926 - lr: 1.6777e-20\n",
      "Epoch 79/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2810 - accuracy: 0.4274\n",
      "Epoch 00079: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 3.3554436145371517e-21.\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2812 - accuracy: 0.4273 - val_loss: 148.6415 - val_accuracy: 0.2926 - lr: 1.6777e-20\n",
      "Epoch 80/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2737 - accuracy: 0.4280\n",
      "Epoch 00080: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2738 - accuracy: 0.4279 - val_loss: 131.6831 - val_accuracy: 0.2926 - lr: 3.3554e-21\n",
      "Epoch 81/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2748 - accuracy: 0.4281\n",
      "Epoch 00081: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2745 - accuracy: 0.4281 - val_loss: 155.6662 - val_accuracy: 0.2917 - lr: 3.3554e-21\n",
      "Epoch 82/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2723 - accuracy: 0.4286\n",
      "Epoch 00082: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 6.710887229074304e-22.\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2724 - accuracy: 0.4286 - val_loss: 120.7393 - val_accuracy: 0.2933 - lr: 3.3554e-21\n",
      "Epoch 83/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2774 - accuracy: 0.4285\n",
      "Epoch 00083: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2774 - accuracy: 0.4285 - val_loss: 151.7587 - val_accuracy: 0.2916 - lr: 6.7109e-22\n",
      "Epoch 84/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2772 - accuracy: 0.4270\n",
      "Epoch 00084: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2772 - accuracy: 0.4270 - val_loss: 121.2423 - val_accuracy: 0.2931 - lr: 6.7109e-22\n",
      "Epoch 85/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2811 - accuracy: 0.4268\n",
      "Epoch 00085: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 1.3421774862045392e-22.\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2811 - accuracy: 0.4268 - val_loss: 151.9509 - val_accuracy: 0.2923 - lr: 6.7109e-22\n",
      "Epoch 86/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2739 - accuracy: 0.4284\n",
      "Epoch 00086: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2739 - accuracy: 0.4284 - val_loss: 158.9506 - val_accuracy: 0.2916 - lr: 1.3422e-22\n",
      "Epoch 87/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2780 - accuracy: 0.4271\n",
      "Epoch 00087: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.2780 - accuracy: 0.4270 - val_loss: 134.5455 - val_accuracy: 0.2929 - lr: 1.3422e-22\n",
      "Epoch 88/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2725 - accuracy: 0.4279\n",
      "Epoch 00088: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 2.684355073383274e-23.\n",
      "2188/2188 [==============================] - 69s 32ms/step - loss: 2.2724 - accuracy: 0.4279 - val_loss: 132.8084 - val_accuracy: 0.2931 - lr: 1.3422e-22\n",
      "Epoch 89/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2686 - accuracy: 0.4315\n",
      "Epoch 00089: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2687 - accuracy: 0.4315 - val_loss: 140.9487 - val_accuracy: 0.2930 - lr: 2.6844e-23\n",
      "Epoch 90/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2757 - accuracy: 0.4272\n",
      "Epoch 00090: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2757 - accuracy: 0.4272 - val_loss: 171.4951 - val_accuracy: 0.2919 - lr: 2.6844e-23\n",
      "Epoch 91/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2716 - accuracy: 0.4271\n",
      "Epoch 00091: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 5.368710272984293e-24.\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2716 - accuracy: 0.4271 - val_loss: 137.8020 - val_accuracy: 0.2926 - lr: 2.6844e-23\n",
      "Epoch 92/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2681 - accuracy: 0.4301\n",
      "Epoch 00092: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2684 - accuracy: 0.4300 - val_loss: 116.3715 - val_accuracy: 0.2937 - lr: 5.3687e-24\n",
      "Epoch 93/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2714 - accuracy: 0.4296\n",
      "Epoch 00093: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2714 - accuracy: 0.4296 - val_loss: 136.9656 - val_accuracy: 0.2928 - lr: 5.3687e-24\n",
      "Epoch 94/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2768 - accuracy: 0.4286\n",
      "Epoch 00094: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 1.0737420861512948e-24.\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2768 - accuracy: 0.4286 - val_loss: 129.3544 - val_accuracy: 0.2927 - lr: 5.3687e-24\n",
      "Epoch 95/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2752 - accuracy: 0.4285\n",
      "Epoch 00095: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2754 - accuracy: 0.4285 - val_loss: 126.5355 - val_accuracy: 0.2931 - lr: 1.0737e-24\n",
      "Epoch 96/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2731 - accuracy: 0.4290\n",
      "Epoch 00096: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2730 - accuracy: 0.4290 - val_loss: 136.6558 - val_accuracy: 0.2927 - lr: 1.0737e-24\n",
      "Epoch 97/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2688 - accuracy: 0.4294\n",
      "Epoch 00097: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 2.147484093416499e-25.\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2685 - accuracy: 0.4295 - val_loss: 156.4260 - val_accuracy: 0.2923 - lr: 1.0737e-24\n",
      "Epoch 98/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2814 - accuracy: 0.4276\n",
      "Epoch 00098: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 68s 31ms/step - loss: 2.2815 - accuracy: 0.4276 - val_loss: 139.3635 - val_accuracy: 0.2926 - lr: 2.1475e-25\n",
      "Epoch 99/100\n",
      "2187/2188 [============================>.] - ETA: 0s - loss: 2.2756 - accuracy: 0.4282\n",
      "Epoch 00099: val_loss did not improve from 3.90370\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2754 - accuracy: 0.4282 - val_loss: 140.7286 - val_accuracy: 0.2922 - lr: 2.1475e-25\n",
      "Epoch 100/100\n",
      "2188/2188 [==============================] - ETA: 0s - loss: 2.2737 - accuracy: 0.4291\n",
      "Epoch 00100: val_loss did not improve from 3.90370\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 4.2949679896177717e-26.\n",
      "2188/2188 [==============================] - 69s 31ms/step - loss: 2.2737 - accuracy: 0.4291 - val_loss: 139.0764 - val_accuracy: 0.2930 - lr: 2.1475e-25\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"Resnet50_V2.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 5,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.2,\n",
    "                              patience = 3,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.0001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [ checkpoint,reduce_lr]\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "batch_size=train_generator.batch_size\n",
    "history= model.fit(train_generator,epochs = epochs,\n",
    "                   callbacks = callbacks,\n",
    "                   batch_size=batch_size,\n",
    "                   validation_data = test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# Plotting our loss charts\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "line1 = plt.plot(epochs, val_loss_values, label='Validation/Test Loss')\n",
    "line2 = plt.plot(epochs, loss_values, label='Training Loss')\n",
    "plt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\n",
    "plt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig('Loss_Resnet50_tiny_q1.png', dpi = 1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting our accuracy charts\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "line1 = plt.plot(epochs, val_acc_values, label='Validation/Test Accuracy')\n",
    "line2 = plt.plot(epochs, acc_values, label='Training Accuracy')\n",
    "plt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\n",
    "plt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig('Accuracy_resnet50_q1.png', dpi = 1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sr\n",
    "import pandas as pd\n",
    "\n",
    "y_pred=np.argmax(model.predict(test_generator), axis=1)\n",
    "y_test=test_generator.classes\n",
    "\n",
    "\n",
    "# y_predict,distances1=  face_recognition(x_test_recog,T_distance)\n",
    "# accuracy=accuracy_score(y_test_recog,y_predict)\n",
    "# class_names=[\"Bedroom\",\"Building\",\"Store\",\"Industry\",\"Living room\",\"Kitchen\",\"Highway\",'Coast',\n",
    "#              \"Office\",\"Mountain\",\"Inside city\",\"Forest\",\"Surbub\",\"Street\",\"Opencountry\"]\n",
    "# report=classification_report(y_test,y_pred)\n",
    "\n",
    "# conf=confusion_matrix(y_test,y_pred,normalize=\"true\")\n",
    "# conf_df=pd.DataFrame(conf)\n",
    "# # print(\"\\nFace  accuracy =\",accuracy)\n",
    "# # print(\"\\n Classification report: \\n\",report)\n",
    "# fig=plt.figure(figsize=(15,10))\n",
    "# sr.heatmap(conf_df,annot=True,cmap=\"Blues\")\n",
    "# plt.title(\"Confusion matrix\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 8s 8ms/step - loss: 139.0765 - accuracy: 0.2930\n",
      "Validation Accuracy: 0.29296666383743286\n"
     ]
    }
   ],
   "source": [
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "Loss,accuracy= model.evaluate(test_generator)\n",
    "print(\"Validation Accuracy:\",accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
