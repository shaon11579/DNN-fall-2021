{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3_Question 3_2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP2ZXBTuaRKsqq0vBzl+bOO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaon11579/DNN-fall-2021/blob/main/A3_Question_3_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQVI731Th9H-"
      },
      "source": [
        "3 of 2\n",
        "In [2]:\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "In [7]:\n",
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "--2021-10-12 18:15:57--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
        "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
        "HTTP request sent, awaiting response... 200 OK\n",
        "Length: 248100043 (237M) [application/zip]\n",
        "Saving to: ‘tiny-imagenet-200.zip’\n",
        "\n",
        "tiny-imagenet-200.z 100%[===================>] 236.61M  88.6MB/s    in 2.7s    \n",
        "\n",
        "2021-10-12 18:15:59 (88.6 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
        "\n",
        "In [8]:\n",
        "!unzip -q tiny-imagenet-200.zip && ls tiny-imagenet-200\n",
        "test  train  val  wnids.txt  words.txt\n",
        "In [3]:\n",
        "def mapDirectoryToInt():\n",
        "    ID_DICT = {}\n",
        "    PATH = 'tiny-imagenet-200/wnids.txt'\n",
        "    FILEOPEN = open(PATH,'r')\n",
        "    for i, folder in enumerate(FILEOPEN):\n",
        "        ID_DICT[folder.replace('\\n','')]=i\n",
        "    return ID_DICT\n",
        "In [4]:\n",
        "def getImage(folderName=None):\n",
        "    DATASET_PATH = 'tiny-imagenet-200/'+folderName+'/'\n",
        "    directories = os.listdir(DATASET_PATH)\n",
        "    ONE_HOT_ENCODED = np.eye(len(directories))\n",
        "    MAP_DIC = mapDirectoryToInt()\n",
        "    TRAIN_IMAGES = []\n",
        "    TRAIN_LABELS = []\n",
        "    CLASS = None\n",
        "    for directory in directories:\n",
        "        path = DATASET_PATH+directory+'/'\n",
        "        files = os.listdir(path)\n",
        "        if files[0].endswith('.txt'):\n",
        "            CLASS = files[0]\n",
        "        else:\n",
        "            CLASS = files[1]\n",
        "        for file in files:\n",
        "            if file.endswith('.txt'):\n",
        "                continue\n",
        "            IMG_FOLDER_PATH = path+file+'/'\n",
        "            IMAGES = os.listdir(IMG_FOLDER_PATH)\n",
        "            for IMAGE in IMAGES:\n",
        "                IMG_PATH = IMG_FOLDER_PATH+IMAGE\n",
        "                img = cv2.imread(IMG_PATH)\n",
        "                if len(img.shape)< 3:\n",
        "                    continue\n",
        "                #img = np.reshape(img,[64,64,3])\n",
        "                TRAIN_IMAGES.append(img)\n",
        "                TRAIN_LABELS.append((np.reshape(ONE_HOT_ENCODED[MAP_DIC[CLASS.replace('_boxes.txt','')]], [200])))\n",
        "    return np.array(TRAIN_IMAGES), np.array(TRAIN_LABELS)\n",
        "In [5]:\n",
        "def loadValidationData(folderName=None):\n",
        "    DATASET_PATH = 'tiny-imagenet-200/'\n",
        "    DATASET_FILE_PATH = 'tiny-imagenet-200/'+folderName+'/val_annotations.txt'\n",
        "    ONE_HOT_ENCODED = np.eye(200)\n",
        "    MAP_DIC = mapDirectoryToInt()\n",
        "    file = open(DATASET_FILE_PATH)\n",
        "    TEST = []\n",
        "    TEST_LABELS = []\n",
        "    for line in file:\n",
        "        image, class_id = line.split('\\t')[:2]\n",
        "        IMG_PATH = DATASET_PATH+folderName+'/images/'+image\n",
        "        img = cv2.imread(IMG_PATH)\n",
        "        if len(img.shape) <3:\n",
        "            continue\n",
        "        TEST.append(img)\n",
        "        TEST_LABELS.append((np.reshape(ONE_HOT_ENCODED[MAP_DIC[class_id]], [200])))\n",
        "    return np.array(TEST), np.array(TEST_LABELS)\n",
        "In [9]:\n",
        "from tensorflow import keras\n",
        "TEST_IMAGES, TEST_LABELS = loadValidationData('val')\n",
        "TEST_IMAGES = keras.applications.resnet50.preprocess_input(TEST_IMAGES)\n",
        "In [10]:\n",
        "print(TEST_IMAGES.shape)\n",
        "print(TEST_LABELS.shape)\n",
        "(10000, 64, 64, 3)\n",
        "(10000, 200)\n",
        "In [11]:\n",
        "TRAIN_IMAGES, TRAIN_LABELS = getImage('train')\n",
        "In [12]:\n",
        "TRAIN_IMAGES =  keras.applications.resnet50.preprocess_input(TRAIN_IMAGES)\n",
        "In [13]:\n",
        "path = 'tiny-imagenet-200/'\n",
        "In [14]:\n",
        "import tensorflow.keras as K\n",
        "In [15]:\n",
        "input_shape = K.Input(shape=(64,64,3))\n",
        "In [16]:\n",
        "resModel = ResNet50(include_top=False, weights='imagenet',input_tensor=input_shape)\n",
        "#model.summary()\n",
        "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "94773248/94765736 [==============================] - 1s 0us/step\n",
        "94781440/94765736 [==============================] - 1s 0us/step\n",
        "In [17]:\n",
        "for layer in resModel.layers[:-2]:\n",
        "  layer.trainable = False\n",
        "In [18]:\n",
        "import tensorflow as tf \n",
        "model = K.models.Sequential()\n",
        "model.add(resModel)\n",
        "model.add(K.layers.Flatten())\n",
        "model.add(K.layers.BatchNormalization())\n",
        "model.add(K.layers.Dropout(0.7))\n",
        "\n",
        "model.add(K.layers.Dense(200, activation='softmax'))\n",
        "model.summary()\n",
        "Model: \"sequential\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "resnet50 (Functional)        (None, 2, 2, 2048)        23587712  \n",
        "_________________________________________________________________\n",
        "flatten (Flatten)            (None, 8192)              0         \n",
        "_________________________________________________________________\n",
        "batch_normalization (BatchNo (None, 8192)              32768     \n",
        "_________________________________________________________________\n",
        "dropout (Dropout)            (None, 8192)              0         \n",
        "_________________________________________________________________\n",
        "dense (Dense)                (None, 200)               1638600   \n",
        "=================================================================\n",
        "Total params: 25,259,080\n",
        "Trainable params: 1,654,984\n",
        "Non-trainable params: 23,604,096\n",
        "_________________________________________________________________\n",
        "In [19]:\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=K.optimizers.Adam(learning_rate=0.0001),\n",
        "                  metrics=['accuracy'])\n",
        "In [20]:\n",
        "TRAIN_IMAGES.shape, TRAIN_LABELS.shape, TEST_IMAGES.shape, TEST_LABELS.shape\n",
        "Out[20]:\n",
        "((100000, 64, 64, 3), (100000, 200), (10000, 64, 64, 3), (10000, 200))\n",
        "In [21]:\n",
        "history = model.fit(TRAIN_IMAGES, TRAIN_LABELS, batch_size=32,  epochs=30, verbose=1,\n",
        "                        validation_data=(TEST_IMAGES, TEST_LABELS),shuffle=True)\n",
        "Epoch 1/30\n",
        "3125/3125 [==============================] - 77s 19ms/step - loss: 4.6520 - accuracy: 0.1947 - val_loss: 2.7350 - val_accuracy: 0.4032\n",
        "Epoch 2/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 3.4539 - accuracy: 0.3289 - val_loss: 2.5844 - val_accuracy: 0.4296\n",
        "Epoch 3/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 3.0541 - accuracy: 0.3728 - val_loss: 2.5127 - val_accuracy: 0.4423\n",
        "Epoch 4/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 2.7906 - accuracy: 0.4051 - val_loss: 2.4784 - val_accuracy: 0.4534\n",
        "Epoch 5/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 2.6056 - accuracy: 0.4270 - val_loss: 2.4419 - val_accuracy: 0.4600\n",
        "Epoch 6/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 2.4798 - accuracy: 0.4479 - val_loss: 2.4455 - val_accuracy: 0.4586\n",
        "Epoch 7/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 2.3696 - accuracy: 0.4624 - val_loss: 2.4313 - val_accuracy: 0.4594\n",
        "Epoch 8/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 2.2899 - accuracy: 0.4742 - val_loss: 2.4287 - val_accuracy: 0.4625\n",
        "Epoch 9/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 2.2243 - accuracy: 0.4849 - val_loss: 2.4265 - val_accuracy: 0.4661\n",
        "Epoch 10/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 2.1680 - accuracy: 0.4913 - val_loss: 2.4176 - val_accuracy: 0.4639\n",
        "Epoch 11/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 2.1342 - accuracy: 0.4986 - val_loss: 2.4273 - val_accuracy: 0.4675\n",
        "Epoch 12/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 2.0954 - accuracy: 0.5037 - val_loss: 2.4291 - val_accuracy: 0.4667\n",
        "Epoch 13/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 2.0689 - accuracy: 0.5075 - val_loss: 2.4365 - val_accuracy: 0.4654\n",
        "Epoch 14/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 2.0446 - accuracy: 0.5118 - val_loss: 2.4270 - val_accuracy: 0.4711\n",
        "Epoch 15/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 2.0235 - accuracy: 0.5131 - val_loss: 2.4260 - val_accuracy: 0.4694\n",
        "Epoch 16/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 1.9957 - accuracy: 0.5207 - val_loss: 2.4439 - val_accuracy: 0.4671\n",
        "Epoch 17/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 1.9973 - accuracy: 0.5196 - val_loss: 2.4507 - val_accuracy: 0.4665\n",
        "Epoch 18/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 1.9765 - accuracy: 0.5251 - val_loss: 2.4461 - val_accuracy: 0.4692\n",
        "Epoch 19/30\n",
        "3125/3125 [==============================] - 52s 17ms/step - loss: 1.9625 - accuracy: 0.5252 - val_loss: 2.4562 - val_accuracy: 0.4704\n",
        "Epoch 20/30\n",
        "3125/3125 [==============================] - 54s 17ms/step - loss: 1.9514 - accuracy: 0.5271 - val_loss: 2.4617 - val_accuracy: 0.4689\n",
        "Epoch 21/30\n",
        "3125/3125 [==============================] - 52s 17ms/step - loss: 1.9363 - accuracy: 0.5318 - val_loss: 2.4579 - val_accuracy: 0.4678\n",
        "Epoch 22/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 1.9345 - accuracy: 0.5304 - val_loss: 2.4562 - val_accuracy: 0.4671\n",
        "Epoch 23/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 1.9189 - accuracy: 0.5327 - val_loss: 2.4652 - val_accuracy: 0.4666\n",
        "Epoch 24/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 1.9106 - accuracy: 0.5317 - val_loss: 2.4557 - val_accuracy: 0.4689\n",
        "Epoch 25/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 1.8997 - accuracy: 0.5378 - val_loss: 2.4617 - val_accuracy: 0.4691\n",
        "Epoch 26/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 1.8935 - accuracy: 0.5381 - val_loss: 2.4566 - val_accuracy: 0.4670\n",
        "Epoch 27/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 1.8875 - accuracy: 0.5396 - val_loss: 2.4587 - val_accuracy: 0.4696\n",
        "Epoch 28/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 1.8868 - accuracy: 0.5389 - val_loss: 2.4741 - val_accuracy: 0.4703\n",
        "Epoch 29/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 1.8678 - accuracy: 0.5410 - val_loss: 2.4749 - val_accuracy: 0.4643\n",
        "Epoch 30/30\n",
        "3125/3125 [==============================] - 51s 16ms/step - loss: 1.8739 - accuracy: 0.5396 - val_loss: 2.4777 - val_accuracy: 0.4675\n",
        "In [22]:\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title(\"Model Accuracy(Fine-Tunining)\")\n",
        "plt.show()\n",
        "\n",
        "In [23]:\n",
        "plt.plot(history.history['loss'],'r', label='Training Loss')\n",
        "plt.plot(history.history['val_loss'],'c', label = 'Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title(\"Model Loss(Fine-Tunining)\")\n",
        "plt.show()\n",
        "\n",
        "In [27]:\n",
        "print('Training accuracy: ', np.around(history.history['accuracy'][29]*100, decimals=2),'%')\n",
        "print('Validaiton accuracy: ', np.around(history.history['val_accuracy'][29]*100, decimals=2),'%')\n",
        "Training accuracy:  53.96 %\n",
        "Validaiton accuracy:  46.75 %\n",
        "In [28]:\n",
        "prediction = model.evaluate(TEST_IMAGES,TEST_LABELS)\n",
        "print(\"Testing Accuracy: \", np.around(prediction[1]*100, decimals=2))\n",
        "print(\"Testing Loss: \", np.around(prediction[0],decimals=2))\n",
        "313/313 [==============================] - 5s 16ms/step - loss: 2.4777 - accuracy: 0.4675\n",
        "Testing Accuracy:  46.75\n",
        "Testing Loss:  2.48"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}