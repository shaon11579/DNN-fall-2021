{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"GPU","colab":{"name":"ASSIGNMENT_3_QUESTION_3(a).ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tbb9grNyssJX"},"source":["# Assignment 3 Question 3\n","### Lokesh Chandra Das"],"id":"tbb9grNyssJX"},{"cell_type":"code","metadata":{"id":"LPMRHWiSc4oA"},"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","from tensorflow import keras\n","from matplotlib import pyplot as plt\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D,MaxPooling2D, Dense,Flatten,Dropout, BatchNormalization,Activation\n","from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras import initializers\n","import os\n","import cv2\n","\n","\n","\n","\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","from keras.wrappers.scikit_learn import KerasClassifier"],"id":"LPMRHWiSc4oA","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jRbs9U1Xc_DG","executionInfo":{"status":"ok","timestamp":1633749676007,"user_tz":300,"elapsed":20899,"user":{"displayName":"lokesh akash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFB-YjitDzBCMnfbdlCNDTYyHuDa8Bx__4QVhwAA=s64","userId":"00836425460329919980"}},"outputId":"0d30298d-5789-4700-d55b-854c4dcd86a2"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"jRbs9U1Xc_DG","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"Ho8plcdrdK7K"},"source":["from collections import Counter\n","DATASET_PATH = '/content/drive/MyDrive/Dataset/15-Scene/'\n","directories = os.listdir(DATASET_PATH)\n","ONE_HOT_ENCODED = np.eye(len(directories))\n","FEATURES = []\n","LABELS = []\n","for category in sorted(directories):\n","    path = DATASET_PATH+category\n","    files = os.listdir(path)\n","    for fileName in files:\n","        imgPath = path+\"/\"+fileName\n","        img = cv2.imread(imgPath)\n","        img = cv2.resize(img, (32,32))\n","        #img = np.reshape(img,[32,32,3])\n","        FEATURES.append(img)\n","        LABELS.append((np.reshape(ONE_HOT_ENCODED[int(category)], [15])))"],"id":"Ho8plcdrdK7K","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dhWLGYFWdR7p"},"source":["# Randomly shuffle the datasets\n","shuffle_indices = np.random.permutation(len(FEATURES))\n","FEATURES = np.array(FEATURES)\n","LABELS = np.array(LABELS)\n","FEATURES = FEATURES[shuffle_indices]\n","LABELS = LABELS[shuffle_indices]"],"id":"dhWLGYFWdR7p","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fESpMieajGNW"},"source":["TEST_PERCENTAGE = 0.3 # 30% for testing and 70% for training\n","train_X = FEATURES[:int(len(FEATURES)*(1-TEST_PERCENTAGE))]\n","train_y = LABELS[:int(len(LABELS)*(1-TEST_PERCENTAGE))]\n","print(train_X.shape)"],"id":"fESpMieajGNW","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t_wGfZLIjL4F"},"source":["test_X = FEATURES[int(len(FEATURES)*(1-TEST_PERCENTAGE)):]\n","test_y = LABELS[int(len(LABELS)*(1-TEST_PERCENTAGE)):]\n","print(test_X.shape)"],"id":"t_wGfZLIjL4F","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xcCAE6UnjPnX"},"source":["isAugmentation = True\n","activation='elu'\n","batch_size = 64\n","epochs = 150\n","num_classes = 15\n","INITIALIZER = initializers.GlorotNormal() # xavier initializer\n","SAVE_DIR = os.path.join(os.getcwd(),'models')\n","MODEL_NAME='ASSIGNMENT_2_PART_2.h5'"],"id":"xcCAE6UnjPnX","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LvZqAIUqjUWs"},"source":["train_X = train_X.astype('float32')\n","train_y = train_y.astype('float32')\n","test_X = test_X.astype('float32')\n","test_y = test_y.astype('float32')\n","train_X /= 255.0\n","test_X /= 255.0"],"id":"LvZqAIUqjUWs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f384f8b9"},"source":["def dataAugmentation(isAugmentation=False):\n","    if not isAugmentation:\n","        return None\n","    else:\n","        dataGenerator =  ImageDataGenerator(\n","        featurewise_center=False,  # set input mean to 0 over the dataset\n","        samplewise_center=False,  # set each sample mean to 0\n","        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","        samplewise_std_normalization=False,  # divide each input by its std\n","        zca_whitening=False,  # apply ZCA whitening\n","        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n","        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n","        # randomly shift images horizontally (fraction of total width)\n","        width_shift_range=0.1,\n","        # randomly shift images vertically (fraction of total height)\n","        height_shift_range=0.1,\n","        shear_range=0.,  # set range for random shear\n","        zoom_range=0.,  # set range for random zoom\n","        channel_shift_range=0.,  # set range for random channel shifts\n","        # set mode for filling points outside the input boundaries\n","        fill_mode='nearest',\n","        cval=0.,  # value used for fill_mode = \"constant\"\n","        horizontal_flip=True,  # randomly flip images\n","        vertical_flip=False,  # randomly flip images\n","        # set rescaling factor (applied before any other transformation)\n","        rescale=None,\n","        # set function that will be applied on each input\n","        preprocessing_function=None,\n","        # image data format, either \"channels_first\" or \"channels_last\"\n","        data_format=None,\n","        # fraction of images reserved for validation (strictly between 0 and 1)\n","        validation_split=0.1)\n","        return dataGenerator\n","        "],"id":"f384f8b9","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TAnEeoZOr3DA"},"source":["dataGenerator = dataAugmentation(True)"],"id":"TAnEeoZOr3DA","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"omWOL4bDmV1G"},"source":["# Optimizer RMSProp\n"],"id":"omWOL4bDmV1G"},{"cell_type":"code","metadata":{"id":"9DsuPHTbmaZf"},"source":["def createModel():\n","\n","  weight_decay = 1e-6\n","  model = Sequential()\n","  model.add(Conv2D(32, (5,5), padding='same', kernel_initializer=INITIALIZER, kernel_regularizer=keras.regularizers.l2(weight_decay), input_shape=train_X.shape[1:]))\n","  model.add(Activation('elu'))\n","  model.add(BatchNormalization())\n","  model.add(Conv2D(32, (5,5), padding='same', kernel_initializer=INITIALIZER, kernel_regularizer=keras.regularizers.l2(weight_decay)))\n","  model.add(Activation('elu'))\n","  model.add(BatchNormalization())\n","  model.add(MaxPooling2D(pool_size=(2,2)))\n","  model.add(Dropout(0.25))\n","  \n","  model.add(Conv2D(64, (3,3), padding='same', kernel_initializer=INITIALIZER, kernel_regularizer=keras.regularizers.l2(weight_decay)))\n","  model.add(Activation('elu'))\n","  model.add(BatchNormalization())\n","  model.add(Conv2D(64, (3,3), padding='same', kernel_initializer=INITIALIZER, kernel_regularizer=keras.regularizers.l2(weight_decay)))\n","  model.add(Activation('elu'))\n","  model.add(BatchNormalization())\n","  model.add(MaxPooling2D(pool_size=(2,2)))\n","  model.add(Dropout(0.25))\n","  \n","  model.add(Conv2D(128, (3,3), padding='same', kernel_initializer=INITIALIZER, kernel_regularizer=keras.regularizers.l2(weight_decay)))\n","  model.add(Activation('elu'))\n","  model.add(BatchNormalization())\n","  model.add(Conv2D(128, (3,3), padding='same', kernel_initializer=INITIALIZER, kernel_regularizer=keras.regularizers.l2(weight_decay)))\n","  model.add(Activation('elu'))\n","  model.add(BatchNormalization())\n","  model.add(MaxPooling2D(pool_size=(2,2)))\n","  model.add(Dropout(0.45))\n","  \n","  model.add(Flatten())\n","  model.add(Dense(num_classes, activation='softmax'))\n","  optRMSprop = opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n","  model.compile(loss='categorical_crossentropy', optimizer=optRMSprop, metrics=['accuracy'])\n","  return model\n"],"id":"9DsuPHTbmaZf","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTOGfQkXKyQo"},"source":["indModel = createModel()\n","if dataGenerator is not None:\n","  dataGenerator.fit(train_X)\n","  ind_history = indModel.fit(dataGenerator.flow(train_X, train_y,\n","                                     batch_size=batch_size,subset='training'), \n","                        validation_data=dataGenerator.flow(train_X, train_y,\n","                                     batch_size=batch_size,subset='validation'),verbose=2,epochs=epochs)\n","else:\n"," ind_history = indModel.fit(train_X,train_y, batch_size=batch_size,epochs=epochs,validation_data=(test_X,test_y),verbose=2)"],"id":"mTOGfQkXKyQo","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fh7hyKIcLyzN"},"source":["evaluations = indModel.evaluate(test_X,test_y)\n","print(evaluations)"],"id":"fh7hyKIcLyzN","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D-TJpa3JNl2d"},"source":["print('Single Model Accuracy: ', np.around((evaluations[1]*100),decimals=2),\" %\")"],"id":"D-TJpa3JNl2d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KYmY7_467bnr"},"source":["model = createModel()\n","num_of_model_to_ensemble = 3"],"id":"KYmY7_467bnr","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fbIQ4HXI9-gM"},"source":["def getModel():\n","  allTrainedModels = []\n","  for _ in range(num_of_model_to_ensemble):\n","    model = createModel()\n","    if dataGenerator is not None:\n","      dataGenerator.fit(train_X)\n","      model.fit(dataGenerator.flow(train_X, train_y,\n","                                     batch_size=batch_size,subset='training'), \n","                        validation_data=dataGenerator.flow(train_X, train_y,\n","                                     batch_size=batch_size,subset='validation'),verbose=2,epochs=epochs)\n","    else:\n","      model.fit(train_X,train_y, batch_size=batch_size,epochs=epochs,validation_data=(test_X,test_y),verbose=2)\n","    allTrainedModels.append(model)\n","  return allTrainedModels"],"id":"fbIQ4HXI9-gM","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZTbcjP__gBM"},"source":["ensembleModels = getModel()"],"id":"XZTbcjP__gBM","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wibfdG7FBWvZ"},"source":["def ensemblePrediction(models):\n","  predictions = [model.predict(test_X) for model in models ]\n","  predictions = np.array(predictions)\n","  summed = np.sum(predictions,axis=0)\n","  result = np.argmax(summed,axis=1)\n","  return result"],"id":"wibfdG7FBWvZ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z658PDRbB_ZI"},"source":["prediction = ensemblePrediction(ensembleModels)\n","test_y_arg=np.argmax(test_y,axis=1)\n","accuracy_score(prediction,test_y_arg)"],"id":"Z658PDRbB_ZI","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ntikqmMTVuc-"},"source":[""],"id":"ntikqmMTVuc-","execution_count":null,"outputs":[]}]}